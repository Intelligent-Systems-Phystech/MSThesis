\documentclass[12pt,twoside]{article}
\usepackage[russian,english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{abstract}
\usepackage{amsmath,amssymb,mathrsfs,mathtext,amsthm}
\usepackage{a4wide}
\usepackage[T2A]{fontenc}
\usepackage{subfig}
\usepackage{url}
\usepackage[usenames]{color}
\usepackage{colortbl}

\newcommand{\hdir}{.}
\usepackage{hyperref}       % clickable links
\usepackage{lineno}
\usepackage{graphicx,multicol}
\usepackage{epstopdf}
\usepackage{cite}
\usepackage{amsmath,amssymb,mathrsfs,mathtext}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,shadows}
\newtheorem{theorem}{Theorem}
\newtheorem{statement}{Statement}
\usepackage{algorithm}
\usepackage[noend]{algcompatible}

%\renewcommand{\baselinestretch}{1.4}


\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\bchi}{\boldsymbol{\chi}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\T}{^{\text{\tiny\sffamily\upshape\mdseries T}}}
\newcommand{\argmin}{\mathop{\arg \min}\limits}
\newcommand{\argmax}{\mathop{\arg \max}\limits}

\newcommand\undermat[2]{%
	\makebox[0pt][l]{$\smash{\underbrace{\phantom{%
					\begin{matrix}#2\end{matrix}}}_{\text{$#1$}}}$}#2}


\begin{document}

\linenumbers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem statement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The goal is to forecast a dependent target variable $\by \in \bbR^r$ from a independent input object $\bx \in \bbR^n$.
We assume that there is a linear dependence between the objects $\bx$ and the target vector $\by$
\begin{equation}
	 \by = \bTheta \bx+ \boldsymbol{\varepsilon}, 
	\label{eq:model}
\end{equation}
where $\bTheta \in \bbR^{r \times n}$ is the matrix of model parameters, $\boldsymbol{\varepsilon} \in \bbR^{r}$ is the vector of residuals.
The task is to find the matrix of the model parameters $\bTheta$ given the dataset $\left( \bX, \bY \right)$, where $\bX \in \bbR^{m \times n}$ is a design matrix, $\bY \in \bbR^{m \times r}$ is a target matrix
\[
	\bX = [\bx_1, \dots, \bx_m]^{\T} =  [\bchi_1, \dots, \bchi_n]; \quad \bY = [\by_1, \dots, \by_m]^{\T} =  [\bphi_1, \dots, \bphi_r].
\]
The examples of how to construct the dataset for a particular application task are described in the \hyperref[sec:exper]{Computational experiment}.

The optimal parameters are determined by minimization of an error function. 
Define the quadratic error function:
\begin{equation}
	S(\bTheta | \bX, \bY) = {\left\| \underset{m \times n}{\vphantom{\by}\bX} \cdot \underset{n \times r}{\vphantom{\by}\bTheta} - \underset{m \times r}{\vphantom{\by}\mathbf{Y}} \right\| }_2^2 = \sum_{i=1}^m \left\| \underset{1 \times n}{\vphantom{\by}\bx_i} \cdot \underset{n \times r}{\vphantom{\by}\bTheta} - \underset{1 \times r}{\vphantom{\by}\by_i} \right\|_2^2 \rightarrow\min_{\bTheta}.
\label{eq:error_function}
\end{equation}
 
 The solution of the problem~\eqref{eq:error_function} is given by
 \[
 	\bTheta = (\bX^{\T} \bX)^{-1} \bX^{\T} \bY.
 \]
 
 The linear dependence of the matrix $\bX$ columns leads to an instable solution for the optimization problem~\eqref{eq:error_function}. 
 If there is a vector $\boldsymbol{\alpha} \neq 0$ such that $\bX \boldsymbol{\alpha}= 0$ than adding the vector~$\boldsymbol{\alpha}$ to any column of the matrix~$\bTheta$ does not change the error function $S(\bTheta | \bX, \bY)$.
 In this case the matrix $\bX^{\T} \bX$ is not invertible.
 To avoid the strong linear dependence feature selection and dimensionality reduction techniques are used.
 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \section{Feature selection}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 The goal of feature selection is to find the index set~$\cA = \{1, \dots, n\}$ of matrix $\bX$ columns. To select the set~$\cA$ among all possible $2^n - 1$ subsets, introduce the feature selection quality criteria
\begin{equation}
	\cA = \argmax_{\cA' \subseteq \{1, \dots, n\}} Q(\cA' | \bX, \bY).
	\label{eq:subset_selection}
\end{equation}

\subsection{Quadratic Programming Feature Selection}
One of the approach to the feature selection is to maximize feature relevances and minimize pairwise feature redundancy.
The QPFS algorithm selects non-correlated features, which are relevant to the target vector~$\bphi$ for the linear regression problem ($r=1$)
\begin{equation*}
	\| \bphi - \bX \btheta\|_2^2 \rightarrow\min_{\btheta \in \bbR^{n}}.
\end{equation*}
Introduce two functions: $\text{Sim}(\bX)$ and $\text{Rel}(\bX, \bphi)$. 
The $\text{Sim}(\bX)$ measures the redundancy between features, the $\text{Rel}(\bX, \bphi)$ contains relevances between each feature and the target vector~$\bphi$. 
We want to minimize the function Sim and maximize the Rel simultaneously.

QPFS offers the explicit way to construct the functions Sim and Rel. 
The method minimizes the following functional
\begin{equation}
	(1 - \alpha) \cdot \underbrace{\ba^{\T} \bQ \ba}_{\text{Sim}} - \alpha \cdot \underbrace{\vphantom{()} \mathbf{b}^{\T} \ba}_{\text{Rel}} \rightarrow \min_{\substack{\ba \in \bbR^n_+ \\ \|\ba\|_1=1}}.
	\label{eq:quadratic_problem}
\end{equation}
The matrix $\bQ \in \bbR^{n \times n}$ entries measure the pairwise similarities between features. 
The vector $\mathbf{b} \in \bbR^n$ expresses the similarities between each feature and the target matrix~$\bb$.
The normalized vector~$\ba$ shows the importance of each feature. 
The functional~\eqref{eq:quadratic_problem} penalizes the dependent features by the function Sim and encourages features relevant to the target by the function Rel. 
The parameter~$\alpha$ allows to control the trade-off between the functions Sim and the Rel.
The authors of the original QPFS paper suggested the way to select~$\alpha$ and make $\text{Sim}(\bX)$ and $\text{Rel}(\bX, \bphi)$ impact the same
\begin{equation*}
	\alpha = \frac{\overline{\bQ}}{\overline{\bQ} + \overline{\bb}},
\end{equation*}
where $\overline{\bQ}$, $\overline{\bb}$ are the mean values of~$\bQ$ and $\bb$ respectively.
Apply the thresholding for~$\ba$ to find the optimal feature subset:
\[
	j \in \mathcal{A} \Leftrightarrow a_j > \tau.
\]

To measure similarity the authors use the absolute value of sample correlation coefficient between pairs of features for the function Sim, and between features and the target matrix~$\bphi$ for the function Rel
\begin{equation}
	\bQ = \left\{\left|\text{corr}(\bchi_i, \bchi_j)\right|\right\}_{i,j=1}^n, \quad \bb = \left\{\left|\text{corr}(\bchi_i, \bphi)\right|\right\}_{i=1}^n.
	\label{eq:qpfs_1d_qb}
\end{equation}
The problem~\eqref{eq:quadratic_problem} is convex if the matrix~$\bQ$ is positive semidefinite. In general it is not always true. 
To satisfy this condition the matrix~$\bQ$ spectrum is shifted and the matrix~$\bQ$ is replaced by $\bQ - \lambda_{\text{min}} \mathbf{I}$, where $\lambda_{\text{min}} $ is a $\bQ$ minimal eigenvalue.

The functional~\eqref{eq:quadratic_problem} corresponds to the quality criteria~$Q(\cA | \bX, \bphi)$
\begin{equation}
\cA = \argmax_{\cA' \subseteq \{1, \dots, n\}} Q(\cA' | \bX, \bphi) \Leftrightarrow \argmin_{\ba  \in \bbR^n_+, \, \|\ba\|_1=1} \bigl[\ba^{\T} \bQ \ba - \alpha \cdot \mathbf{b}^{\T} \ba \bigr].
\end{equation}

\subsection{Multivariate QPFS}
First of approach to apply the QPFS algorithm to the case, when $\bY$ is a matrix ($r > 1$) is to aggregate feature relevances throuch all $r$ components. The term $\text{Sim}(\bX)$ is still the same, and the matrix $\bQ$ and the vector $\bb$ are equal to
\begin{equation*}
\bQ = \left\{\left|\text{corr}(\bchi_i, \bchi_j)\right|\right\}_{i,j=1}^n, \quad \bb = \left\{\sum_{k=1}^r\left|\text{corr}(\bchi_i, \bphi_k)\right|\right\}_{i=1}^n.
\end{equation*}

ЭТО ПРИМЕР

This approach does not use the dependencies in the columns of the matrix $\bY$. Let consider the following case.
\[
	\bX = [\bchi_1, \bchi_2, \bchi_3], \quad \bY = [\underbrace{\bphi_1, \bphi_1, \dots, \bphi_1}_{r-1}, \bphi_2],
\]
We have three features and $r$ targets, where first $r-1$ target are the same. 
The pairwise features similarities are given by the matrix $\bQ$. 
Matrix $\bB$ entries shows pairwise relevances features to the targets. 
The vector $\bb$ is obtained by summation of the matrix $\bB$ over columns.
\[
	\bQ = \begin{bmatrix} 1 & 0 & 0\\ 0 & 1 & 0.8 \\ 0 & 0.8 & 1 \end{bmatrix}, \quad 
	\bB = \begin{bmatrix} 0.4 & \dots & 0.4 & 0 \\ 0.5 & \dots & 0.5 & 0.8 \\ \undermat{r-1}{0.8 & \dots & 0.8} & 0.1 \end{bmatrix}, \quad
	\bb = \begin{bmatrix} (r-1) \cdot 0.4 + 0 \\ (r-1) \cdot 0.5 + 0.8 \\ (r-1) \cdot 0.8 + 0.1 \end{bmatrix}
\]
	\vspace{0.5cm}

We would like to select only two features.
For such configuration the best feature subset is~$[\bchi_1, \bchi_2]$. 
The feature~$\bchi_2$ predicts the second target~$\bphi_2$ and the combination of features~$\bchi_1, \bchi_2$ predict the first component. 
The QPFS algorithm for~$r=2$ gives the solution~$\ba = [0.46, 0.31, 0.23]$. It coincides with our knowledge. However, if we add the collinear columns to the matrix~$\bY$ and increase~$r$ to 5, the QPFS solution will be~$\ba = [0.46, 0.25, 0.29]$. Here we lost the relevant feature~$\bchi_2$ and select the redundant feature~$\bchi_3$.

КОНЕЦ ПРИМЕРА

To take into account the dependencies in the columns of the matrix $\bY$ we extend the QPFS functional~\eqref{eq:quadratic_problem} to the multivariate case. We add the term~$\text{Sim}(\bY)$ and extend the term $\text{Rel}(\bX, \bY)$:
\begin{equation}
	\alpha_1 \cdot \underbrace{\ba_x^{\T} \bQ_x \ba_x}_{\text{Sim}(\bX)} - \alpha_2 \cdot \underbrace{\ba_x^{\T} \bB \ba_y}_{\text{Rel}(\bX, \bY)} + \alpha_3 \cdot \underbrace{\ba_y^{\T} \bQ_y \ba_y}_{\text{Sim}(\bY)} \rightarrow \min_{\substack{\ba_x \in \bbR^n_+ \, \|\ba_x\|_1=1 \\ \ba_y \in \bbR^r _+ \, \|\ba_y\|_1=1}}.
\end{equation}
Determine the entries of matrices $\bQ_x \in \bbR^{n \times n}$, $\bQ_y \in \bbR^{r \times r}$, $\bB \in \bbR^{n \times r}$ in the following way
\begin{equation}
	\bQ_x = \left\{ \left| \text{corr}(\bchi_i, \bchi_j) \right| \right\}_{i,j=1}^n, \quad 
	\bQ_y = \left\{ \left| \text{corr}(\bphi_i, \bphi_j) \right| \right\}_{i,j=1}^r, \quad
	\bB =  \left\{ \left| \text{corr}(\bchi_i, \bphi_j) \right| \right\}_{\substack{i=1, \dots, n \\ j=1, \dots, r}}.
	\label{eq:multivariate_quadratic_problem}
\end{equation}
The coefficients $\alpha_1$, $\alpha_2$, and $\alpha_3$ control the influence of each term to the functional~\eqref{eq:multivariate_quadratic_problem} and satisfy the conditions:
\[
	\alpha_i \geq 0, \, i = 1, 2, 3; \quad \alpha_1 + \alpha_2 + \alpha_3 = 1.
\]
For the case $r=1$ the proposed approach coincides with the original QPFS algorithm.

\section{Feature categorization}
Feature selection algorithms eliminate features which are not relevant to the target variable. 
To determine whether the feature is relevant the t-test could be applied for the correlation coefficient.
\[
	r = \text{corr} (\bchi, \bphi), \quad t = \frac{r \sqrt{m - 2}}{1 - r^2} \sim \text{St} (m - 2).
\]
\begin{align*}
&H_0: r = 0 \\
&H_1: r \neq 0
\end{align*}
If features are relevant, but correlated, feature selection methods pick the subset of them to reduce the multicollinearity and redundancy.
The goal is to find relevant, non-correlated features. 
However, in this case the correlations between targets in $\bY$ matrix are crucial.
To measure the dependence of each feature or target, the Variance Inflation Factor is computed
\[
	\text{VIF}(\bchi_j) = \frac{1}{1 - R_j^2}, \quad \text{VIF}(\bphi_k) = \frac{1}{1 - R_k^2},
\]
where $R_j^2$($R_k^2$) are coefficients of determination for the regression of $\bchi_j$($\bphi_k$) on the other features(targets).

On that basis, we categorize features into 5 disjoint groups:
\begin{enumerate}
	\item non-relevant features
	\[
		\left\{j: \text{corr}(\bchi_j, \bphi_k) = 0, \, \forall k \in \{1, \dots, r\}\right\};
	\]
	\item non-$\bX$-correlated features, which are relevant to non-$\bY$-correlated targets
	\[
		\left\{j: \left(\text{VIF}(\bchi_j) < 10\right) \, \text{and} \, \left(\text{VIF}(\bphi_k) < 10 , \, \forall k \in \{1, \dots, r\}: \,  \text{corr}(\bchi_j, \bphi_k) \neq 0 \right)\right\};
	\]
	\item non-$\bX$-correlated features, which are relevant to $\bY$-correlated targets
	\[
		\left\{j: \left(\text{VIF}(\bchi_j) < 10\right) \, \text{and} \, \left( \exists k \in \{1, \dots, r\}: \text{VIF}(\bphi_k) > 10 \,\, \& \,\, \text{corr}(\bchi_j, \bphi_k) \neq 0 \right)\right\};
	\]
	\item $\bX$-correlated features, which are relevant to non-$\bY$-correlated targets
	\[
		\left\{j: \left(\text{VIF}(\bchi_j) > 10\right) \, \text{and} \, \left(\text{VIF}(\bphi_k) < 10 , \, \forall k \in \{1, \dots, r\}: \,  \text{corr}(\bchi_j, \bphi_k) \neq 0 \right)\right\};
	\]
	\item $\bX$-correlated features, which are relevant to $\bY$-correlated targets
	\[
		\left\{j: \left(\text{VIF}(\bchi_j) > 10\right) \, \text{and} \, \left( \exists k \in \{1, \dots, r\}: \text{VIF}(\bphi_k) > 10 \,\, \& \,\, \text{corr}(\bchi_j, \bphi_k) \neq 0 \right)\right\}.
	\]
\end{enumerate}

\end{document}