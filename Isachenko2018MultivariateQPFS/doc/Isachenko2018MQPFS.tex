\documentclass[12pt,twoside]{article}
\usepackage[russian,english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{abstract}
\usepackage{amsmath,amssymb,mathrsfs,mathtext,amsthm}
\usepackage{a4wide}
\usepackage[T2A]{fontenc}
\usepackage{subfig}
\usepackage{url}
\usepackage[usenames]{color}
\usepackage{colortbl}

\newcommand{\hdir}{.}
\usepackage{hyperref}       % clickable links
\usepackage{lineno}
\usepackage{graphicx,multicol}
\usepackage{epstopdf}
\usepackage{cite}
\usepackage{amsmath,amssymb,mathrsfs,mathtext}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,shadows}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\usepackage{algorithm}
\usepackage[noend]{algcompatible}

\usepackage{multirow}

\usepackage{caption}

%\renewcommand{\baselinestretch}{1.4}


\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\T}{\mathsf{T}}
\newcommand{\bchi}{\boldsymbol{\chi}}
\newcommand{\bnu}{\boldsymbol{\nu}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\bOne}{\boldsymbol{1}}
\newcommand{\bZero}{\boldsymbol{0}}
\newcommand{\argmin}{\mathop{\arg \min}\limits}
\newcommand{\argmax}{\mathop{\arg \max}\limits}

\newcommand\undermat[2]{%
	\makebox[0pt][l]{$\smash{\underbrace{\phantom{%
					\begin{matrix}#2\end{matrix}}}_{\text{$#1$}}}$}#2}

\begin{document}
	
	\linenumbers
	
	\title
{Multivariate Quadratic Programming Feature Selection for ECoG signal decoding}
\date{}
\maketitle
\begin{center}
	R.\,V.~Isachenko,
	V.\,V.~Strijov
\end{center}
\textbf{Abstract:} 
The paper is devoted to the problem of signal decoding for Brain Computer Interface. 
The goal is to build a model which predicts a limb position by brain signals. 
The challenge of the investigation is redundancy in data description. 
High correlation in measurements leads to correlation in input space. Additionally, the target space are correlated due to dependent consequent hand positions.
To overcome multicorrelation in feature representation, feature selection are used.
However, the majority of feature selection methods ignore the dependencies in the target space.
The authors suggest a novel approach to feature selection in multivariate regression.
To take into account the correlations in the target matrix, the proposed approach extend the ideas of the quadratic programming feature selection~(QPFS) algorithm. 
The QPFS algorithm select non-correlated features, which are relevant to the targets. The proposed methods weigh the targets by their importances.
The computational experiment shows high performance of the proposed algorithms in the ECoG data.

\bigskip
\textbf{Keywords}: multivariate regression, quadratic programming feature selection, ECoG signal decoding

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The paper investigates the problem of signal decoding for Brain Computer Interface~(BCI)~\cite{costecalde2018long}. 
The BCI aims to develop systems that help people with a severe motor control disability to recover mobility.
The minimally-invasive implant records cortical signals and the model decodes them on real time to predict the coordinates of an exoskeleton limbs~\cite{mestais2015wimagine,eliseyev2014clinatec}.
The subject placed inside the exoskeleton can drive it by imagining movements as if they were making the movement by themselves. 

The challenge to build such model is redundancy in initial data description. 
The features are highly multicorrelated due to spatial nature of the data. 
The brain sensors are close to each other. 
It leads to the redundant measurements and instability of the final model.
In addition, the redundant data description requires excess computations which lead to real-time delay. 
To overcome this problem feature selection methods are used~\cite{katrutsa2015stress,li2017feature}.

One of the approach to the feature selection is to maximize feature relevances and minimize pairwise feature redundancy. 
This approach was recently proposed in~\cite{ding2005minimum}.
It was shown in~\cite{rodriguez2010quadratic,katrutsa2017comprehensive} that quadratic programming feature selection~(QPFS) outperforms many existing feature selection methods for the univariate regression problem. 
The QPFS algorithm introduces two functions: $\text{Sim}$ and $\text{Rel}$.
The $\text{Sim}$ measures the redundancy between features, the $\text{Rel}$ contains relevances between each feature and the target vector.
We want to minimize the function Sim and maximize the Rel simultaneously.
QPFS offers the explicit way to construct the functions Sim and Rel.
The method minimizes the following function
\begin{equation}
(1 - \alpha) \cdot \underbrace{\bz^{\T} \bQ \bz}_{\text{Sim}} - \alpha \cdot \underbrace{\vphantom{()} \bb^{\T} \bz}_{\text{Rel}} \rightarrow \min_{\substack{\bz \geq \bZero_n \\ \bOne_n^{\T} \bz=1}}.
\label{eq:quadratic_problem}
\end{equation}
The matrix $\bQ \in \bbR^{n \times n}$ entries measure the pairwise similarities between features.
The vector $\bb \in \bbR^n$ expresses the similarities between each feature and the target vector.
The normalized vector~$\bz$ shows the importance of each feature.
The function~\eqref{eq:quadratic_problem} penalizes the dependent features by the function Sim and encourages features relevant to the target by the function Rel.
The parameter~$\alpha$ controls the trade-off between the functions Sim and the Rel.
To measure similarity the authors use the absolute value of sample correlation coefficient or sample mutual information coefficient between pairs of features for the function Sim, and between features and the target vector for the function Rel.

We consider the multivariate problem, where the dependent variable is a vector. 
It refers to the prediction of limb position for not just one moment, but for some period of time. 
The subsequent hand positions are correlated. 
It leads to correlations in the model output. 
In this situation feature selection algorithms do not take into account these dependencies.
Hence, the selected feature subset is not optimal.
We propose methods to take into account the dependencies in both input and output spaces. 
It allows to get the stable model with fewer variables.

The experiment was carried out in the ECoG data from the NeuroTycho project~\cite{neurotycho}. 
The proposed algorithms outperforms the original methods.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem statement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The goal is to forecast a dependent variable $\by \in \bbR^r$ with $r$ targets from an independent input object $\bx \in \bbR^n$ with $n$ features.
We assume there is a linear dependence
\begin{equation}
	\by = \bTheta \bx+ \boldsymbol{\varepsilon}
	\label{eq:model}
\end{equation}
between the object $\bx$ and the target variable $\by$,
where $\bTheta \in \bbR^{r \times n}$ is a matrix of model parameters, $\boldsymbol{\varepsilon} \in \bbR^{r}$ is a residual vector.
One has to find the matrix of the model parameters $\bTheta$ given a dataset $\left( \bX, \bY \right)$, where $\bX \in \bbR^{m \times n}$ is a design matrix, $\bY \in \bbR^{m \times r}$ is a target matrix
\[
	\bX = [\bx_1, \dots, \bx_m]^{\T} =  [\bchi_1, \dots, \bchi_n]; \quad \bY = [\by_1, \dots, \by_m]^{\T} =  [\bnu_1, \dots, \bnu_r].
\]
The columns~$\bchi_j$ of the matrix~$\bX$ respond to object features.

The optimal parameters are determined by minimization of an error function.
Define the quadratic loss function:
\begin{equation}
	\mathcal{L}(\bTheta | \bX, \bY) = {\left\| \underset{m \times r}{\mathbf{Y}}  - \underset{m \times n}{\bX} \cdot \underset{r \times n}{\bTheta}^{\T} \right\| }_2^2 \rightarrow\min_{\bTheta}.
\label{eq:error_function}
\end{equation}

 The solution of~\eqref{eq:error_function} is given by
 \[
 	\bTheta = \bY^{\T} \bX (\bX^{\T} \bX)^{-1}.
 \]

 The linear dependent columns of the matrix $\bX$ leads to an instable solution for the optimization problem~\eqref{eq:error_function}.
 If there is a vector $\boldsymbol{\alpha} \neq \bZero_n$ such that $\bX \boldsymbol{\alpha}= \bZero_m$, then adding the vector~$\boldsymbol{\alpha}$ to any column of the matrix~$\bTheta$ does not change the value of the loss function $\mathcal{L}(\bTheta | \bX, \bY)$.
 In this case the matrix $\bX^{\T} \bX$ is not invertible.
 To avoid the strong linear dependence, feature selection and dimensionality reduction techniques are used.

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \section{Feature selection}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 The feature selection goal is to find the boolean vector~$\ba = \{0, 1\}^n$, which components indicates whether the feature are selected. To obtain the optimal vector~$\ba$ among all possible $2^n - 1$ options, introduce the feature selection error function
\begin{equation}
	\ba = \argmin_{\ba' \in \{0, 1\}^n} S(\ba' | \bX, \bY).
	\label{eq:subset_selection}
\end{equation}
The goal of feature selection is to construct the appropriate function~$S(\ba | \bX, \bY)$. The particular examples for the considered feature selection algorithms are given below and summarizes in the Table~\ref{tbl:summary}.

Once the solution~$\ba$ of~\eqref{eq:subset_selection} is known, the problem~\eqref{eq:error_function} becomes
\begin{equation}
\mathcal{L}(\bTheta_{\ba} | \bX_{\ba}, \bY) = {\left\| \mathbf{Y} - \bX_{\ba}\bTheta^{\T}_{\ba} \right\| }_2^2 \rightarrow\min_{\bTheta_{\ba}},
\end{equation}
where the subscript~$\ba$ indicates the submatrix with the columns for which components of~$\ba$ equal 1.

\subsection{Quadratic Programming Feature Selection}
The QPFS algorithm selects non-correlated features, which are relevant to the target vector~$\bnu$ for the linear regression problem with~$r=1$
\begin{equation*}
	\| \bnu - \bX \btheta\|_2^2 \rightarrow\min_{\btheta \in \bbR^{n}}.
\end{equation*}
The domain of QPFS function~\eqref{eq:quadratic_problem} is $[0, 1]^n$. It refers to the relaxed error function~$S(\ba | \bX, \bnu)$, which has boolean domain~$\{0, 1\}^n$. 
The link between the boolean vector~$\ba$ from~\eqref{eq:subset_selection} and the QPFS vector~$\bz$ from~\eqref{eq:quadratic_problem} is given by:
\[
\ba = [a_j]_{j=1}^n, \quad 
a_j = \begin{cases}
1, & z_j > \tau; \\
0, & \text{otherwise}.
\end{cases}
\]
The authors of the original QPFS paper suggested the way to select~$\alpha$ and make $\text{Sim}(\bX)$ and $\text{Rel}(\bX, \bnu)$ impacts the same:
\begin{equation*}
	\alpha = \frac{\overline{\bQ}}{\overline{\bQ} + \overline{\bb}},
\end{equation*}
where $\overline{\bQ}$, $\overline{\bb}$ are the mean values of~$\bQ$ and $\bb$ respectively.
We use the absolute value of sample correlation coefficient as similarity measure:
\begin{equation}
	\bQ = \left[\left|\text{corr}(\bchi_i, \bchi_j)\right|\right]_{i,j=1}^n, \quad \bb = \left[\left|\text{corr}(\bchi_i, \bnu)\right|\right]_{i=1}^n.
	\label{eq:qpfs_1d_qb}
\end{equation}
The other choices to define $\bQ$ and $\bb$, such as mutual information and normalized feature significance, are considered in~\cite{katrutsa2017comprehensive}.

The problem~\eqref{eq:quadratic_problem} is convex if the matrix~$\bQ$ is positive semidefinite. In general it is not always true.
To satisfy this condition, the matrix~$\bQ$ spectrum is shifted and the matrix~$\bQ$ is replaced by $\bQ - \lambda_{\text{min}} \mathbf{I}$, where $\lambda_{\text{min}} $ is a $\bQ$ minimal eigenvalue.


\subsection{Multivariate QPFS}

We are aimed to propose the algorithms which suitable for feature selection in multivariate case. 
If the target space is multidimensional it prone to redundancy and correlations between the targets. 
In this section we consider the algorithms that take into account the probable dependencies in both input and target spaces.

\paragraph{Relevance aggregation (RelAgg).}

First approach to apply the QPFS algorithm to the multivariate case ($r > 1$) is to aggregate feature relevances through all $r$ components. The term $\text{Sim}(\bX)$ is still the same, the matrix $\bQ$ is defined by~\eqref{eq:qpfs_1d_qb}. The vector $\bb$ is aggregated across all targets and is defined by
\begin{equation*}
\bb = \left[\sum_{k=1}^r\left|\text{corr}(\bchi_i, \bnu_k)\right|\right]_{i=1}^n.
\end{equation*}

The drawback of this approach is that it does not use the dependencies in the columns of the matrix $\bY$. Observe the following example:
\[
	\bX = [\bchi_1, \bchi_2, \bchi_3], \quad \bY = [\underbrace{\bnu_1, \bnu_1, \dots, \bnu_1}_{r-1}, \bnu_2],
\]
We have three features and $r$ targets, where first $r-1$ target are identical.
The pairwise features similarities are given by the matrix $\bQ$.
The matrix $\bB$ entries show pairwise features relevances to the targets.
The vector $\bb$ is obtained by summation of the matrix $\bB$ over columns.
\begin{equation}
	\bQ = \begin{bmatrix} 1 & 0 & 0\\ 0 & 1 & 0.8 \\ 0 & 0.8 & 1 \end{bmatrix}, \quad
	\bB = \begin{bmatrix} 0.4 & \dots & 0.4 & 0 \\ 0.5 & \dots & 0.5 & 0.8 \\ \undermat{r-1}{0.8 & \dots & 0.8} & 0.1 \end{bmatrix}, \quad
	\bb = \begin{bmatrix} (r-1) \cdot 0.4 + 0 \\ (r-1) \cdot 0.5 + 0.8 \\ (r-1) \cdot 0.8 + 0.1. \end{bmatrix}
	\label{eq:qpfs_example}
\end{equation}
	\vspace{0.5cm} \\
We would like to select only two features.
For such configuration the best feature subset is~$[\bchi_1, \bchi_2]$.
The feature~$\bchi_2$ predicts the second target~$\bnu_2$ and the combination of features~$\bchi_1, \bchi_2$ predicts the first component.
The QPFS algorithm for~$r=2$ gives the solution~$\bz = [0.37,	0.61,	0.02]$. It coincides with our knowledge.
However, if we add the collinear columns to the matrix~$\bY$ and increase~$r$ to 5, the QPFS solution will be~$\bz = [0.40,	0.17, 0.43]$.
Here we lose the relevant feature~$\bchi_2$ and select the redundant feature~$\bchi_3$.

\paragraph{Symmetric importances (SymImp).}

To take into account the dependencies in the columns of the matrix $\bY$ we extend the QPFS function~\eqref{eq:quadratic_problem} to the multivariate case.
We add the term~$\text{Sim}(\bY)$ and modify the term $\text{Rel}(\bX, \bY)$:
\begin{equation}
	\alpha_1 \cdot \underbrace{\bz_x^{\T} \bQ_x \bz_x}_{\text{Sim}(\bX)} - \alpha_2 \cdot \underbrace{\bz_x^{\T} \bB \bz_y}_{\text{Rel}(\bX, \bY)} + \alpha_3 \cdot \underbrace{\bz_y^{\T} \bQ_y \bz_y}_{\text{Sim}(\bY)} \rightarrow \min_{\substack{\bz_x \geq \bZero_n, \, \bOne_n^{\T}\bz_x=1 \\ \bz_y \geq \bZero_r, \, \bOne_r^{\T}\bz_y=1}}.
	\label{eq:multivariate_quadratic_problem}
\end{equation}
Determine the entries of matrices $\bQ_x \in \bbR^{n \times n}$, $\bQ_y \in \bbR^{r \times r}$, $\bB \in \bbR^{n \times r}$ in the following way
\begin{equation*}
	\bQ_x = \left[ \left| \text{corr}(\bchi_i, \bchi_j) \right| \right]_{i,j=1}^n, \quad
	\bQ_y = \left[ \left| \text{corr}(\bnu_i, \bnu_j) \right| \right]_{i,j=1}^r, \quad
	\bB =  \left[ \left| \text{corr}(\bchi_i, \bnu_j) \right| \right]_{\substack{i=1, \dots, n \\ j=1, \dots, r}}.
\end{equation*}
The vector~$\bz_x$ shows the feature importances, while $\bz_y$ is a vector with the importances of the targets.
The correlated targets will be penalized by $\text{Sim} (\bY)$ and have the lower importances.

The coefficients $\alpha_1$, $\alpha_2$, and $\alpha_3$ control the influence of each term on the function~\eqref{eq:multivariate_quadratic_problem} and satisfy the conditions:
\[
\alpha_1 + \alpha_2 + \alpha_3 = 1, \quad \alpha_i \geq 0, \, i = 1, 2, 3.
\]
\begin{proposition}
	The balance between the terms~$\text{Sim}(\bX)$, $\text{Rel}(\bX, \bY)$, and $\text{Rel}(\bX, \bY)$ for the problem~\eqref{eq:multivariate_quadratic_problem} is achieved by the following coefficients:
	\[
	\alpha_1 = \frac{\overline{\bQ}_y \overline{\bB} }{\overline{\bQ}_y \overline{\bB} + \overline{\bQ}_x \overline{\bQ}_y + \overline{\bQ}_x \overline{\bB}}; \quad
	\alpha_2 = \frac{\overline{\bQ}_x \overline{\bQ}_y}{\overline{\bQ}_y \overline{\bB} + \overline{\bQ}_x \overline{\bQ}_y + \overline{\bQ}_x \overline{\bB}}; \quad
	\alpha_3  = \frac{\overline{\bQ}_x \overline{\bB}}{\overline{\bQ}_y \overline{\bB} + \overline{\bQ}_x \overline{\bQ}_y + \overline{\bQ}_x \overline{\bB}},
	\]
	where $\overline{\bQ}_x$, $\overline{\bB}$, $\overline{\bQ}_y$ are mean values of~$\bQ_x$, $\bB$, and $\bQ_y$, respectively.

\end{proposition}
\begin{proof}
	The desired values of $\alpha_1$, $\alpha_2$, and $\alpha_3$ are given by solution of the following equations
	\begin{align*}
		&\alpha_1 + \alpha_2 + \alpha_3 = 1; \\
		&\alpha_1 \overline{\bQ}_x = \alpha_2 \overline{\bB} = \alpha_3 \overline{\bQ}_y.
	\end{align*}
	Here, the mean values~$\overline{\bQ}_x$, $\overline{\bB}$, $\overline{\bQ}_y$ of the corresponding matrices ~$\bQ_x$, $\bB$, and $\bQ_y$ are the mean values of the terms~$\text{Sim}(\bX)$, $\text{Rel}(\bX, \bY)$, and $\text{Rel}(\bX, \bY)$.
\end{proof}
To investigate the impact of the term $\text{Sim}(\bY)$ on the function~\eqref{eq:multivariate_quadratic_problem}, we balance the terms $\text{Sim}(\bX)$ and $\text{Rel}(\bX, \bY)$ by fixing the proportion between~$\alpha_1$ and $\alpha_2$:
\begin{equation}
\alpha_1 = \frac{(1 - \alpha_3)\overline{\bB}}{\overline{\bQ}_x + \overline{\bB}}; \quad
\alpha_2 = \frac{(1 - \alpha_3)\overline{\bQ}_x}{\overline{\bQ}_x + \overline{\bB}}; \quad
\alpha_3 \in [0, 1].
\label{eq:alphas3}
\end{equation}

We apply the proposed algorithm to the discussed example~\eqref{eq:qpfs_example}.
The given matrix~$\bQ$ corresponds to the matrix~$\bQ_x$.
We additionally define the matrix~$\bQ_y$ by setting $\text{corr}(\bnu_1, \bnu_2) = 0.2$ and all others entries to one.
Figure~\ref{fig:features_vs_alpha} shows the importances of features~$\bz_x$ and targets~$\bz_y$ with respect to~$\alpha_3$ coefficient.
If~$\alpha_3$ is small, the impact of all targets are almost equal and the feature~$\bchi_3$ dominates the feature~$\bchi_2$. When~$\alpha_3$ becomes larger than~$0.2$, the importance~$(\bz_y)_5$ of the target~$\phi_5$ grows up along with the importance of the feature~$\bchi_2$.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/features_vs_alpha.pdf}
	\caption{Feature importances $\bz_x$ and $\bz_y$ with respect to the $\alpha_3$ coefficient}
	\label{fig:features_vs_alpha}
\end{figure}

\paragraph{Minimax QPFS (MinMax and MaxMin).}
The function~\eqref{eq:multivariate_quadratic_problem} is symmetric with respect to~$\bz_x$ and $\bz_y$.
It penalizes features that are correlated and do not relevant to targets.
At the same time it penalizes targets that are correlated and are not sufficiently explained by the features.
It leads to small importances for targets which are difficult to predict by features and large importances for targets which are strongly correlated with features.
It contradicts with the intuition.
Our goal is to predict all targets, especially which are difficult to explain, by selected relevant and non-correlated features. We express this into two related problems.
\begin{align}
	\alpha_1 \cdot \underbrace{\bz_x^{\T} \bQ_x \bz_x}_{\text{Sim}(\bX)} - \alpha_2 \cdot \underbrace{\vphantom{()} \bz_x^{\T}\mathbf{B} \bz_y}_{\text{Rel}(\bX, \bY)} \rightarrow \min_{\substack{\bz_x \geq \bZero_n, \\ \bOne_n^{\T}\bz_x=1}};
	\label{eq:x_qpfs}\\
	\alpha_3 \cdot \underbrace{\bz_y^{\T} \bQ_y \bz_y}_{\text{Sim}(\bY)} + \alpha_2 \cdot \underbrace{\vphantom{()} \bz_x^{\T} \mathbf{B} \bz_y}_{\text{Rel}(\bX, \bY)} \rightarrow \min_{\substack{\bz_y \geq \bZero_r,  \\ \bOne_r^{\T}\bz_y=1}}.
	\label{eq:y_qpfs}
\end{align}
The difference is in the term Rel.
In feature space the non-relevant components should have smaller scores.
Meanwhile, the targets that are not relevant to the features should have larger scores.
The problems~\eqref{eq:x_qpfs} and \eqref{eq:y_qpfs} are merged into the joint min-max or max-min formulation
\begin{equation}
	\min_{\substack{\bz_x \geq \bZero_n \\ \bOne_n^{\T}\bz_x=1}} 	\max_{\substack{\bz_y \geq \bZero_r \\ \bOne_r^{\T}\bz_y=1}} f(\bz_x, \bz_y), \quad \left(\text {or} \, \max_{\substack{\bz_y \geq \bZero_r \\ \bOne_r^{\T}\bz_y=1}} \min_{\substack{\bz_x \geq \bZero_n \\ \bOne_n^{\T}\bz_x=1}} f(\bz_x, \bz_y)\right),
	\label{eq:minmax}
\end{equation}
where
\[
	f(\bz_x, \bz_y) = \alpha_1 \cdot \underbrace{\bz_x^{\T} \bQ_x \bz_x}_{\text{Sim}(\bX)} - \alpha_2 \cdot \underbrace{\bz_x^{\T} \bB \bz_y}_{\text{Rel}(\bX, \bY)} - \alpha_3 \cdot \underbrace{\bz_y^{\T} \bQ_y \bz_y}_{\text{Sim}(\bY)}.
\]
\begin{theorem}
	For positive definite matrices $\bQ_x$ and $\bQ_y$ the max-min and min-max problems~\eqref{eq:minmax} have the same optimal value.
\end{theorem}
\begin{proof}
	Denote
	\begin{equation*}
	\mathbb{C}^n = \{\bz : \bz \geq \bZero_n, \, \bOne_n^{\T}\bz=1\}, \quad \mathbb{C}^r = \{\bz : \bz \geq \bZero_r, \, \bOne_r^{\T}\bz=1\}.
	\end{equation*}
	The sets $\mathbb{C}^n$ and $\mathbb{C}^r$ are compact and convex. The function $f: \mathbb{C}^n \times \mathbb{C}^r \rightarrow \bbR$ is a continuous function. If $\bQ_x$ and $\bQ_y$ are positive definite matrices, the function~$f$ is convex-concave, i.e.
	$f(\cdot, \bz_y): \mathbb{C}^n \rightarrow \bbR$ is convex for fixed~$\bz_y$, and $f(\bz_x, \cdot): \mathbb{C}^r \rightarrow \bbR$ is concave for fixed~$\bz_x$.
	In this case Neumann's minimax theorem states
	\[
	\min_{\bz_x \in \mathbb{C}^n} \max_{\bz_y \in \mathbb{C}^r} f(\bz_x, \bz_y) = \max_{\bz_y \in \mathbb{C}^r} \min_{\bz_x\in \mathbb{C}^n} f(\bz_x, \bz_y).
	\]
\end{proof}

To solve the min-max problem~\eqref{eq:minmax}, fix some~$\bz_x \in \mathbb{C}^n$. For fixed vector~$\bz_x$ we solve the problem
\begin{equation}
	\max_{\bz_y \in \mathbb{C}_r} f(\bz_x, \bz_y) = \max_{\substack{\bz_y \geq \bZero_r \\ \bOne_r^{\T}\bz_y=1}} \bigl[\alpha_1 \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha_2 \cdot \bz_x^{\T} \bB \bz_y - \alpha_3 \cdot \bz_y^{\T} \bQ_y \bz_y \bigr].
	\label{eq:fixed_ax}
\end{equation}
The Lagrangian for this problem is
\[
	L(\bz_x, \bz_y, \lambda, \bmu) = \alpha_1 \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha_2 \cdot \bz_x^{\T} \bB \bz_y - \alpha_3 \cdot \bz_y^{\T} \bQ_y \bz_y + \lambda \cdot  (\bOne_r^{\T} \bz_y - 1) + \bmu^{\T} \bz_y.
\]
Here the Lagrange multipliers $\bmu$, corresponding to the inequality constraints $\bz_y \geq \bZero_r$, are restricted to be non-negative.
The dual problem is
\begin{equation}
	\min_{\lambda, \, \bmu \geq \bZero_r} g(\bz_x, \lambda, \bmu) = \min_{\lambda, \, \bmu \geq \bZero_r}  \left[\max_{\bz_y \in \bbR^r} L(\bz_x, \bz_y, \lambda, \bmu) \right].
	\label{eq:dual}
\end{equation}
The strong duality holds for~\eqref{eq:fixed_ax}. Therefore, the optimal value for~\eqref{eq:fixed_ax} equals the optimal value for~\eqref{eq:dual}. It allows to solve the problem
\begin{equation}
	\min_{\bz_x \in \mathbb{C}^n, \, \lambda, \, \bmu \geq \bZero_r} g(\bz_y, \lambda, \bmu)
	\label{eq:dual_maxmin}
\end{equation}
instead of~\eqref{eq:minmax}.

Setting the gradient of the Langrangian $\nabla_{\bz_y} L(\bz_x, \bz_y, \lambda, \bmu)$ to zero, we obtain an optimal value~$\bz_y$:
\begin{equation}
	\bz_y = \frac{1}{2\alpha_3} \bQ_y^{-1} \left( - \alpha_2 \cdot \bB^{\T} \bz_x +\lambda \cdot \bOne_r + \bmu \right).
	\label{eq:ax}
\end{equation}
The dual function is equal to
\begin{multline}
	g(\bz_x, \lambda, \bmu)
	= \max_{\bz_y \in \bbR^r} L(\bz_x, \bz_y, \lambda, \bmu) =
	\bz_x^{\T} \left( - \frac{\alpha_2^2}{4\alpha_3} \cdot \bB \bQ_y^{-1} \bB^{\T} - \alpha_1 \cdot \bQ_x\right) \bz_x \\ - \frac{1}{4 \alpha_3} \lambda^2 \cdot \bOne_r^{\T} \bQ_y^{-1} \bOne_r - \frac{1}{4 \alpha_3} \cdot \bmu^{\T} \bQ_y^{-1} \bmu + \frac{\alpha_2}{2 \alpha_3} \lambda \cdot \bOne_r^{\T} \bQ_y^{-1} \bB^{\T} \bz_x \\ - \frac{1}{2 \alpha_3} \lambda \cdot \bOne_r^{\T} \bQ_y^{-1} \bmu + \frac{\alpha_2}{2 \alpha_3} \cdot \bmu^{\T} \bQ_y^{-1} \bB^{\T} \bz_x + \lambda.
	 \label{eq:dual_quadratic_form}
\end{multline}
It brings to the quadratic problem~\eqref{eq:dual_maxmin} with~$n + r + 1$ variables.

\paragraph{Minimax Relevances (MaxRel).}

The problem~\eqref{eq:dual_maxmin} is not convex. If we shift the spectrum for the matrix of quadratic form~\eqref{eq:dual_quadratic_form}, the optimality is lost. To overcome this problem, we drop the term~$\text{Sim}(\bY)$.

\begin{equation}
\min_{\substack{\bz_x \geq \bZero_n \\ \bOne_n^{\T}\bz_x=1}} 	\max_{\substack{\bz_y \geq \bZero_r \\ \bOne_r^{\T}\bz_y=1}} \left[ (1 - \alpha) \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha \cdot \bz_x^{\T} \bB \bz_y \right].
\label{eq:minmax_rel}
\end{equation}
The Lagrangian for the problem~\eqref{eq:minmax_rel} with the fixed vector~$\bz_x$ is
\[
L(\bz_x, \bz_y, \lambda, \bmu) = (1 - \alpha) \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha \cdot \bz_x^{\T} \bB \bz_y + \lambda \cdot  (\bOne_r^{\T} \bz_y - 1) + \bmu^{\T} \bz_y.
\]
Setting the gradient of the Langrangian $\nabla_{\bz_y} L(\bz_x, \bz_y, \lambda, \bmu)$ to zero, we obtain:
\begin{equation*}
\alpha \cdot \bB^{\T} \bz_x = \lambda \cdot \bOne_r + \bmu.
\end{equation*}
The dual function is equal to
\begin{equation}
g(\bz_x, \lambda, \bmu) =
\begin{cases}
(1 - \alpha) \cdot \bz_x^{\T} \bQ_x \bz_x - \lambda, & \alpha \cdot \bB^{\T} \bz_x = \lambda \cdot \bOne_r + \bmu;  \\
+ \infty, & \text{otherwise}.
\end{cases}
\end{equation}
In this case the feature scores are the solution of~\eqref{eq:dual_maxmin}.

\begin{proposition}
	For the case $r=1$ the proposed functions~\eqref{eq:multivariate_quadratic_problem},~\eqref{eq:minmax}, and~\eqref{eq:minmax_rel} coincide with the original QPFS algorithm~\eqref{eq:quadratic_problem}.

	\begin{proof}
		If $r$ is equal to 1, then $\bQ_y = q_y$ is a scalar, $\bz_y = 1$, $\bB = \bb$. It reduces the problems~\eqref{eq:multivariate_quadratic_problem},~\eqref{eq:minmax}, and~\eqref{eq:minmax_rel} to
		\[
		\alpha_1 \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha_2 \cdot \bz_x^{\T} \bb \rightarrow \min_{\bz_x \geq \bZero_n, \, \bOne_n^{\T}\bz_x=1} .
		\]
		Setting $\alpha = \frac{\alpha_2}{\alpha_1 + \alpha_2}$ brings to the original QPFS problem~\eqref{eq:quadratic_problem}.
	\end{proof}
\end{proposition}

To summarize all proposed strategies for multivariate feature selection, Table~\ref{tbl:summary} shows the core ideas and error functions for each method. 

\begin{table}
	\centering
	\small{
	\begin{tabular}{c|c|c}
		\hline
		Algorithm & Strategy & Error function $S(\ba | \bX, \bY)$ \\
		\hline && \\ [-.5em]
		RelAgg & $\min \bigl[ \text{Sim}(\bX) - \text{Rel}(\bX, \bY) \bigr] $ & $\min\limits_{\bz_x} \bigl[ (1 - \alpha) \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha \cdot \bz_x^{\T} \bB \bOne_r \bigr] $ \\ &&\\[-.5em]
		SymImp & $\begin{aligned} \min \, \bigl[ \text{Sim}(\bX) & - \text{Rel}(\bX, \bY) \\ & + \text{Sim}(\bY) \bigr] \end{aligned}$ & $ \min\limits_{\bz_x, \, \bz_y} \left[ \alpha_1 \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha_2 \cdot \bz_x^{\T} \bB \bz_y + \alpha_3 \cdot \bz_y^{\T} \bQ_y \bz_y \right] $\\ &&\\ [-.5em]
		MinMax & $\begin{aligned} &\min \, \bigl[ \text{Sim}(\bX) - \text{Rel}(\bX, \bY) \bigr]  \\ & \max \bigl[\text{Rel}(\bX, \bY) + \text{Sim}(\bY) \bigr] \end{aligned}$ & $	\min\limits_{\bz_x} 	\max\limits_{\bz_y} \bigl[\alpha_1 \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha_2 \cdot \bz_x^{\T} \bB \bz_y - \alpha_3 \cdot \bz_y^{\T} \bQ_y \bz_y \bigr]$ \\ &&\\ 
		MaxMin & $\begin{aligned} &\max \bigl[\text{Rel}(\bX, \bY) + \text{Sim}(\bY) \bigr] \\ & \min \, \bigl[ \text{Sim}(\bX) - \text{Rel}(\bX, \bY) \bigr]  \end{aligned}$ & $\max\limits_{\bz_y} \min\limits_{\bz_x} \bigl[\alpha_1 \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha_2 \cdot \bz_x^{\T} \bB \bz_y - \alpha_3 \cdot \bz_y^{\T} \bQ_y \bz_y \bigr]$\\ &&\\ [-.5em]
		MaxRel & $\begin{aligned} &\min \, \bigl[ \text{Sim}(\bX) - \text{Rel}(\bX, \bY) \bigr]  \\ & \max \bigl[\text{Rel}(\bX, \bY) \bigr] \end{aligned}$& $\min\limits_{\bz_x} 	\max\limits_{\bz_y} \bigl[ (1 - \alpha) \cdot \bz_x^{\T} \bQ_x \bz_x - \alpha \cdot \bz_x^{\T} \bB \bz_y \bigr]$ \\ 
		\hline
	\end{tabular}}
	\caption{Overview of proposed multivariate QPFS algorithms}
	\label{tbl:summary}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To evaluate the selected subset~$\cA$ we introduce criteria that estimates the quality of feature selection procedure.
We measure multicorrelation by mean value of miltiple correlation coefficient as follows
\[
	R^2 = \frac{1}{r} \text{tr} \left( \bC^{\T} \mathbf{R}^{-1} \bC \right); \quad \text{where }\bC = [ \text{corr}(\bchi_i, \bnu_j)]_{\substack{i=1, \dots, n \\ j=1, \dots, r}}, \, \mathbf{R} = [ \text{corr}(\bchi_i, \bchi_j)]_{i, j = 1}^n.
\]
This coefficient lies between 0 and 1. The bigger $R^2$ means the better feature subset we have.

The model stability is given by the logarithm of the ratio between minimal eigenvalue~$\lambda_{\min}$ and maximum eigenvalue~$\lambda_{\max}$ of the matrix~$\bX^{\T} \bX$:
\[
	\text{Stability} = \ln \frac{\lambda_{\min}}{\lambda_{\max}}.
\]
The Root Mean Squared Error (RMSE) shows the quality of the model prediction. We estimate RMSE on train and test data.
\[
	\text{RMSE}(\bY, \widehat{\bY}_{\cA}) = \sqrt{\text{MSE} (\bY, \widehat{\bY}_{\cA})} =  \| \bY - \widehat{\bY}_{\cA} \|_2, \quad \text{where} \quad \widehat{\bY}_{\cA} = \bX_{\cA} \bTheta_{\cA}^{\T}.
\]
Akaike Information Criteria (AIC) is a trade-off between prediction quality and the size of selected subset~$\cA$:
\[
	\text{AIC} = m \ln \left( \frac{\text{MSE} ( \bY, \widehat{\bY}_{\cA})}{m}\right) + 2 | \cA |.
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We carried out computational experiment with ECoG data from the NeuroTycho project. The input data consists of brain voltage signals recorded from 32 channels.
The goal is to predict 3D hand position in the next moments given the input signal.
The example of input signals and the 3D wrist coordinates are shown in Figure~\ref{fig:ecog_data}.
The initial voltage signals are transformed to the spatial-temporal representation using wavelet transformation.
The procedure of extracting feature representation from the raw data are described in details in~\cite{chao2010long,eliseyev2016penalized}.
Feature description at each time moment has dimension equals to 32 (channels) $\times$ 27 (frequencies) = 864.
Each object is the representation of local history time segment with duration $\Delta t = 1s$. The time step between objects is $\delta t =  0.05s$.
The final matrices are $\bX \in \bbR^{18900 \times 864}$ and $\bY \in \bbR^{18900 \times 3k}$, where $k$ is a number of timestamps that we predict.
We split our data into train and test parts with the ratio 0.67.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/ecog_data}
	\caption{Brain signals and the corresponding hand position}
	\label{fig:ecog_data}
\end{figure}

Figures~\ref{fig:feature_scores_ex} and~\ref{fig:train_test_qpfs} show the result of the QPFS algorithm, where we use the Relevance Aggregation strategy and $k = 1$.
QPFS scores~$\bz_x$ decrease sharply.
Only about one hundred features have scores significantly greater than zero.
The test error stops to decrease using more than this amount of features.
It confirms that the initial data representation is highly redundant.
\begin{figure}[h]
	\begin{minipage}{.5\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/feature_scores_ex.eps}
		\caption{Sorted feature importances for \\ the QPFS algorithm}
		\label{fig:feature_scores_ex}
	\end{minipage}%
	\begin{minipage}{.5\linewidth}
	\centering
	\includegraphics[width=\linewidth]{figs/train_test_qpfs.eps}
	\caption{RMSE w.r.t. size of active set, features are ranked by QPFS algorithm}
	\label{fig:train_test_qpfs}
	\end{minipage}
\end{figure}

Figure~\ref{fig:corr_matrix} shows the dependencies in the matrices~$\bX$ and~$\bY$. Frequencies in the matrix~$\bX$ are highly correlated.
The correlations between axes are not significant in comparison with the correlations between consequent moments.
\begin{figure}[h]
	\includegraphics[width=\linewidth]{figs/corr_matrix.eps}
	\caption{Correlation matrices for $\bX$ and $\bY$}
	\label{fig:corr_matrix}
\end{figure}

We apply the QPFS algorithm with Relevance Aggregation strategy for different values of~$\alpha_3$ coefficient according to formulas~\eqref{eq:alphas3}.
The dependence between target scores~$\bz_y$ with respect to~$\alpha_3$ for different values of~$k$ is shown in Figure~\ref{fig:features_vs_alpha_ecog}.
If we predict wrist coordinates only for one timestamp $k = 1$, targets scores are almost the same.
It tells about the independence between $x$, $y$, and $z$ coordinates.
For $k = 2$ and $k = 3$ the scores of some targets become zero when $\alpha_3$ increases.

\begin{figure}[h]
	\begin{minipage}{.5\linewidth}
		\subfloat[k=1]{\label{fig:autoreg_step1}
			\includegraphics[width=\linewidth]{figs/features_vs_alpha_ecog_3.pdf}}
	\end{minipage}%
	\begin{minipage}{.5\linewidth}
		\subfloat[k=2]{\label{fig:autoreg_step2}
			\includegraphics[width=\linewidth]{figs/features_vs_alpha_ecog_6.pdf}}
	\end{minipage}\par\medskip
	\subfloat[k=3]{\label{fig:autoreg_step3}
		\includegraphics[width=\linewidth]{figs/features_vs_alpha_ecog_9.pdf}}

	\caption{Target importances~$\bz_y$ with respect to~$\alpha_3$ for QPFS with Relevance Aggregation}
	\label{fig:features_vs_alpha_ecog}
\end{figure}

We compare the proposed strategies of multivariate QPFS that are given in Table~\ref{tbl:summary} for the ECoG dataset. 
Firstly, we apply all methods to get feature scores. 
Then we fit linear model with increasing number of used features. 
For each method the features are sorted by the obtained scores. 
We show how the described metrics are changed with the increasing feature set size. 
Figure~\ref{fig:ecog_3_1_metrics} illustrates the results for $k = 1$. 
Here all metrics values for RelAgg, SymImp, and MaxRel are quite similar. 
However, MaxMin and MinMax algorithms show worse performance. 
This behavious is possibly due to unreasonable penalty on the matrix~$\bY$.
\begin{figure}[h]
	\includegraphics[width=\linewidth]{figs/ecog_3_1_metrics.pdf}
	\caption{Metrics values for ECoG data with k = 1}
	\label{fig:ecog_3_1_metrics}
\end{figure}

The situation changes for predicting several timestamps. 
In this case the correlation in~$\bY$ matrix is crucial. 
Figure~\ref{fig:ecog_3_15_metrics} shows algorithms performance for $k = 15$. 
The test error is minimal for SymImp, MinMax, and MaxRel strategies.  
The RelAgg strategy shows almost the worst error rate.

\begin{figure}[h]
	\includegraphics[width=\linewidth]{figs/ecog_3_15_metrics.pdf}
	\caption{Metrics values for ECoG data with k = 15}
	\label{fig:ecog_3_15_metrics}
\end{figure}
\begin{table}[]
	\centering
	\begin{tabular}{l|ccccc}
		\hline
		& RMSE  & $\|\ba\|_0$ & Spearman $\rho$ & $\ell_2$ dist \\ \hline
		RelAgg & 41.9 $\pm$ 0.1 & 27.0 $\pm$ 2.4 & 0.941 $\pm$ 0.005 & 0.14 $\pm$ 0.02   \\
		SymImp & 41.1 $\pm$ 0.1 & 198.6 $\pm$ 3.8 & 0.942 $\pm$ 0.008 & 0.03 $\pm$ 0.00   \\
		MinMax & 41.4 $\pm$ 0.2 & 92.4 $\pm$ 8.2 & 0.933 $\pm$ 0.007 & 0.10 $\pm$ 0.01   \\
		MaxMin & 41.7 $\pm$ 0.1 & 97.0 $\pm$ 4.4 & 0.950 $\pm$ 0.004 & 0.07 $\pm$ 0.01   \\
		MaxRel & 41.7 $\pm$ 0.0 & 37.6 $\pm$ 1.6 & 0.893 $\pm$ 0.012 & 0.17 $\pm$ 0.02  \\ \hline
	\end{tabular}
\end{table}

\section{Conclusion}
The paper investigates the problem of signal decoding in relation to Brain Computer Interface. 
To build a stable edequate model, the authors proposed multivariate feature selection strategies. 
The algorithms incorporates dependecies in both input and output spaces. 
The final model are more robust and effective.
The computational experiments shows that the proposed algorithms outperfroms the baseline strategy by multiple metrics. 

\bibliographystyle{unsrt}
\bibliography{papers_qpfs}

\end{document}
